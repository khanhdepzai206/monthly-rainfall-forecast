import pandas as pd
import numpy as np
import pickle
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import matplotlib.pyplot as plt

def evaluate_model(csv_path, model_path, test_size=0.2, random_state=42):
    """
    ÄÃ¡nh giÃ¡ mÃ´ hÃ¬nh vá»›i train/test split
    
    Args:
        csv_path: ÄÆ°á»ng dáº«n file dá»¯ liá»‡u monthly
        model_path: ÄÆ°á»ng dáº«n lÆ°u mÃ´ hÃ¬nh
        test_size: Tá»· lá»‡ test set (default 20%)
        random_state: Random seed
    
    Returns:
        dict: Chá»©a metrics (MAE, RMSE, RÂ², predictions)
    """
    
    # Load dá»¯ liá»‡u
    df = pd.read_csv(csv_path)
    
    # TÃ¡ch input vÃ  output
    X = df[["year", "month"]]
    y = df["rainfall"]
    
    # Train/Test split
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=test_size, random_state=random_state
    )
    
    print(f"ğŸ“Š Dataset Split:")
    print(f"   Training samples: {len(X_train)}")
    print(f"   Testing samples: {len(X_test)}")
    print(f"   Test size: {test_size*100}%\n")
    
    # Táº¡o vÃ  train mÃ´ hÃ¬nh
    model = RandomForestRegressor(n_estimators=200, random_state=random_state)
    model.fit(X_train, y_train)
    
    # Dá»± Ä‘oÃ¡n
    y_train_pred = model.predict(X_train)
    y_test_pred = model.predict(X_test)
    
    # TÃ­nh metrics
    train_mae = mean_absolute_error(y_train, y_train_pred)
    test_mae = mean_absolute_error(y_test, y_test_pred)
    
    train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))
    test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))
    
    train_r2 = r2_score(y_train, y_train_pred)
    test_r2 = r2_score(y_test, y_test_pred)
    
    # In káº¿t quáº£
    print("=" * 50)
    print("ğŸ“ˆ MODEL EVALUATION RESULTS")
    print("=" * 50)
    
    print("\nğŸ¯ TRAINING SET METRICS:")
    print(f"   MAE  (Mean Absolute Error)   : {train_mae:.4f} mm")
    print(f"   RMSE (Root Mean Squared Error): {train_rmse:.4f} mm")
    print(f"   RÂ²   (Coefficient of Determination): {train_r2:.4f}")
    
    print("\nğŸ§ª TESTING SET METRICS:")
    print(f"   MAE  (Mean Absolute Error)   : {test_mae:.4f} mm")
    print(f"   RMSE (Root Mean Squared Error): {test_rmse:.4f} mm")
    print(f"   RÂ²   (Coefficient of Determination): {test_r2:.4f}")
    
    print("\nğŸ“Š MODEL QUALITY INTERPRETATION:")
    if test_r2 > 0.8:
        print(f"   âœ… Excellent model (RÂ² > 0.8)")
    elif test_r2 > 0.6:
        print(f"   âœ… Good model (RÂ² > 0.6)")
    elif test_r2 > 0.4:
        print(f"   âš ï¸  Moderate model (RÂ² > 0.4)")
    else:
        print(f"   âŒ Weak model (RÂ² < 0.4)")
    
    print("\n" + "=" * 50)
    
    # Feature importance
    print("\nğŸ” FEATURE IMPORTANCE:")
    feature_importance = dict(zip(X.columns, model.feature_importances_))
    for feature, importance in sorted(feature_importance.items(), key=lambda x: x[1], reverse=True):
        print(f"   {feature}: {importance:.4f}")
    
    # Save model
    with open(model_path, "wb") as f:
        pickle.dump(model, f)
    print(f"\nâœ… Model saved: {model_path}")
    
    # Return metrics
    metrics = {
        'train_mae': train_mae,
        'test_mae': test_mae,
        'train_rmse': train_rmse,
        'test_rmse': test_rmse,
        'train_r2': train_r2,
        'test_r2': test_r2,
        'X_test': X_test,
        'y_test': y_test,
        'y_test_pred': y_test_pred,
        'model': model
    }
    
    return metrics

def plot_predictions(y_test, y_pred, output_file="predictions_plot.png"):
    """
    Váº½ biá»ƒu Ä‘á»“ so sÃ¡nh giÃ¡ trá»‹ thá»±c vs dá»± Ä‘oÃ¡n
    """
    plt.figure(figsize=(12, 6))
    
    # Plot thá»±c táº¿ vs dá»± Ä‘oÃ¡n
    plt.scatter(y_test, y_pred, alpha=0.5, s=30)
    
    # ÄÆ°á»ng y=x (perfect prediction)
    min_val = min(y_test.min(), y_pred.min())
    max_val = max(y_test.max(), y_pred.max())
    plt.plot([min_val, max_val], [min_val, max_val], 'r--', lw=2, label='Perfect prediction')
    
    plt.xlabel('Actual Rainfall (mm)', fontsize=12)
    plt.ylabel('Predicted Rainfall (mm)', fontsize=12)
    plt.title('Actual vs Predicted Rainfall', fontsize=14, fontweight='bold')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig(output_file, dpi=300)
    print(f"ğŸ“Š Plot saved: {output_file}")
    plt.close()

def plot_residuals(y_test, y_pred, output_file="residuals_plot.png"):
    """
    Váº½ biá»ƒu Ä‘á»“ pháº§n dÆ° (Residuals)
    """
    residuals = y_test - y_pred
    
    plt.figure(figsize=(12, 5))
    
    # Residuals vs predicted values
    plt.subplot(1, 2, 1)
    plt.scatter(y_pred, residuals, alpha=0.5, s=30)
    plt.axhline(y=0, color='r', linestyle='--', lw=2)
    plt.xlabel('Predicted Rainfall (mm)', fontsize=11)
    plt.ylabel('Residuals (mm)', fontsize=11)
    plt.title('Residuals vs Predicted Values', fontsize=12, fontweight='bold')
    plt.grid(True, alpha=0.3)
    
    # Residuals distribution
    plt.subplot(1, 2, 2)
    plt.hist(residuals, bins=30, edgecolor='black', alpha=0.7)
    plt.xlabel('Residuals (mm)', fontsize=11)
    plt.ylabel('Frequency', fontsize=11)
    plt.title('Distribution of Residuals', fontsize=12, fontweight='bold')
    plt.grid(True, alpha=0.3, axis='y')
    
    plt.tight_layout()
    plt.savefig(output_file, dpi=300)
    print(f"ğŸ“Š Plot saved: {output_file}")
    plt.close()

if __name__ == "__main__":
    # ÄÃ¡nh giÃ¡ mÃ´ hÃ¬nh
    metrics = evaluate_model(
        csv_path="data/monthly_rainfall.csv",
        model_path="models/rainfall_model.pkl"
    )
    
    # Váº½ biá»ƒu Ä‘á»“
    plot_predictions(metrics['y_test'], metrics['y_test_pred'], "models/predictions_plot.png")
    plot_residuals(metrics['y_test'], metrics['y_test_pred'], "models/residuals_plot.png")
    
    print("\nâœ… Evaluation complete!")
